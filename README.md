# Azure Data Engineering: End-to-End Lakehouse Pipeline  
**Cloud-Native | Scalable | Secure | Cost-Optimized**
 
[![Azure](https://img.shields.io/badge/Azure-0078D4?logo=microsoftazure&logoColor=white)](https://azure.microsoft.com) 
[![Databricks](https://img.shields.io/badge/Databricks-DB5C5C?logo=databricks&logoColor=white)](https://databricks.com) 
[![Power BI](https://img.shields.io/badge/Power_BI-F2C811?logo=powerbi&logoColor=black)](https://powerbi.microsoft.com)
 
## Project Overview  
Production-ready **Azure data platform** demonstrating full lifecycle:  
**Ingest → Store → Transform → Analyze → Visualize**
 
### Architecture Highlights  
| Layer       | Technology                  | Responsibility                              |  
|-------------|-----------------------------|---------------------------------------------|  
| **Bronze**  | ADF + Data Lake Gen2        | Raw ingestion (JSON, CSV, Parquet)          |  
| **Silver**  | Databricks + Delta Lake     | Cleansed, partitioned, schema-enforced      |  
| **Gold**    | Synapse Analytics           | Aggregated, business-ready dimensional model|  
 
### Key Achievements  
- **Zero-downtime ingestion** via ADF retry logic and dead-letter queues  
- **60% faster analytics** with Synapse result-set caching and materialized views  
- **35% cost reduction** using spot instances and auto-terminating clusters  
- **Full audit trail** with Delta Lake time travel and change data feed (CDF)  
 
### Security & Governance  
- Service Principal + Azure Key Vault for secret management  
- Private Link + VNet integration across all services  
- Column-level encryption and dynamic data masking  
 
**Skills:** Azure Data Engineer | Cloud Architecture | ETL/ELT | PySpark | SQL | Data Modeling | DevOps | Cost Optimization  
